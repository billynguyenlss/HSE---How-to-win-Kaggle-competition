{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "\n",
    "<img src='https://miro.medium.com/max/1575/1*kISLC1Udq0m6g5kwHhMuJg@2x.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://gaussian37.github.io/assets/img/ml/concept/bagging/bagging.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging\n",
    "from sklearn.ensemble import RandomForestRegressor()\n",
    "\n",
    "# train is the training data\n",
    "# test is the test data\n",
    "# y is the target variable\n",
    "model = RandomForestRegressor()\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "# create array object to hold bagged predictions\n",
    "bagged_prediction = np.zeros(test.shape[0])\n",
    "# loop for as many times as we want bags\n",
    "for n in range(bags):\n",
    "    model.set_params(random_state=seed+n) # update seed\n",
    "    model.fit(train,y)\n",
    "    preds = model.predict(test)\n",
    "    bagged_prediction += preds # add predictions to bagged\n",
    "# take average of prediction\n",
    "bagged_prediction /= bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/2936/1*jbncjeM4CfpobEnDO0ZTjw.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight based boosting parameters:**\n",
    "\n",
    "* Learning rate (or shrinkage or eta):\n",
    "\n",
    "predictionN = pred0*eta + pred1*eta + pred2*eta + ...\n",
    "\n",
    "* Number of estimators\n",
    "\n",
    "* Input model - can be anything that accepts weights\n",
    "\n",
    "* Sub boosting type:\n",
    "    \n",
    "    * AdaBoost - Good implementation in sklearn (Python)\n",
    "    \n",
    "    * LogitBoost - Good implementatin in Weka (Java)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting hyperparams tuning:\n",
    "    \n",
    "    - Step 1: choose n_estimators = 100, eta = 0.01\n",
    "    \n",
    "    - Step 2: increase n_estimators = 200, eta = 0.005 (divided by 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual based boosting:**\n",
    "\n",
    "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRLocciBK8NUcR_nuSs4p0F_v4bhS5zUTiK0FX87-hm7q0UuYO9&s'>\n",
    "\n",
    "<img src='https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_400420%2Fimages%2Fx1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning rate (or shrinkage or eta)\n",
    "\n",
    "* Number of estimators\n",
    "\n",
    "* Row (sub) sampling\n",
    "\n",
    "* Column (sub) sampling\n",
    "\n",
    "* Input model - better be trees\n",
    "\n",
    "* Sub boosting type:\n",
    "   \n",
    "   * Fully gradient based\n",
    "   \n",
    "   * Dart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual based favourite implementations**\n",
    "\n",
    "* XGBoost\n",
    "\n",
    "* LightGBM\n",
    "\n",
    "* H2O's GBM\n",
    "\n",
    "* Catboost\n",
    "\n",
    "* Sklearn's GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stacking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Stacking?**\n",
    "\n",
    "<img src='https://miro.medium.com/max/519/1*CixzyDU7lptMbXUXNEZEeA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.researchgate.net/profile/Mahsa_Soufineyestani/publication/326224119/figure/download/fig1/AS:645263400112129@1530854191613/Stacking-model-1-S1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodology:**\n",
    "\n",
    "1. Splitting the train set into 2 disjoint sets\n",
    "\n",
    "2. Train several base learners on the first part\n",
    "\n",
    "3. Make predictions with the base learners on the second (validation) part\n",
    "\n",
    "4. Using the predictions from as the input to train a higher level learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train is the training data\n",
    "# y is the target variable for the train data\n",
    "# test is the test data\n",
    "\n",
    "# Split train data in 2 parts, training and validation\n",
    "training, valid, ytraining, yvalid = train_test_split(train,y,test_size=0.5)\n",
    "\n",
    "# specify models\n",
    "model1 = RandomForestRegressor()\n",
    "model2 = LinearRegressor()\n",
    "\n",
    "# fit models\n",
    "model1.fit(training, ytraining)\n",
    "model2.fit(training, ytraining)\n",
    "\n",
    "# make predictions for validation\n",
    "preds1 = model1.predict(valid)\n",
    "preds2 = model2.predict(valid)\n",
    "\n",
    "# form a new dataset for valid and test via stacking the predictions\n",
    "stacked_predictions = np.column_stack((preds1, preds2))\n",
    "stacked_test_predictions = np.column_stack((test_preds1, test_preds2))\n",
    "\n",
    "# specify metal model\n",
    "meta_model = LinearRegression()\n",
    "# fit meta model on stacked predictions\n",
    "meta_model.fit(stacked_predictions, yvalid)\n",
    "\n",
    "# make predictions on the stacked predictions of the test data\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **StackNET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://zhangruochi.com/Ensemble-Methods/2019/07/17/18.png'>\n",
    "\n",
    "<img src='https://www.researchgate.net/publication/332463178/figure/fig1/AS:748521477140480@1555472835085/The-architecture-of-proposed-StackNet-For-the-ensemble-based-regressor-the-number-of.ppm'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tips and tricks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st level tips**\n",
    "\n",
    "* Diversity based on algorithms:\n",
    "\n",
    "    * 2-3 gradient boosted trees (lightgbm, xgboost, H2O, catboost)\n",
    "    \n",
    "    * 2-3 Neural nets (keras, pytorch)\n",
    "    \n",
    "    * 1-2 ExtraTrees/Random Forest (sklearn)\n",
    "    \n",
    "    * 1-2 linear models as in logistic/ridge regression, linear svm (sklearn)\n",
    "    \n",
    "    * 1-2 knn models (sklearn)\n",
    "    \n",
    "    * 1 Factorization machine (libfm)\n",
    "    \n",
    "    * 1svm with nonlinear kernel if size/memory allows (sklearn)\n",
    "    \n",
    "* Diversity based on input data:\n",
    "\n",
    "    * Categorical features: One hot, label encoding, target encoding\n",
    "    \n",
    "    * Numerical features: outliers, binning, derivatives, percentiles...\n",
    "    \n",
    "    * Interactions: col1*/+- col2, groupby, unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsequent level tips**\n",
    "\n",
    "* Simpler (or shallower) algorithms:\n",
    "\n",
    "    * gradient boosted trees with small depth (like 2 or 3)\n",
    "    \n",
    "    * Linear models with high regularization\n",
    "    \n",
    "    * Extra Trees\n",
    "    \n",
    "    * Shallow networks (as in 1 hidden layer)\n",
    "    \n",
    "    * knn with BrayCurtis Distance\n",
    "    \n",
    "    * Brute forcing a search for best linear weights based on cv\n",
    "    \n",
    "* Feature engineering:\n",
    "\n",
    "    * pairwise differences between meta features\n",
    "    \n",
    "    * row-wise statistics like averages or stds\n",
    "    \n",
    "    * standard feature selection techniques\n",
    "    \n",
    "* For every 7.5 models in previous level we add 1 in meta\n",
    "\n",
    "* Be mindful of target leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Software for Stacking**\n",
    "\n",
    "* StackNet (https://github.com/kaz-Anova/StackNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to validate second level models (meta-models). In this reading material you will find a description for the most popular ones. If not specified, we assume that the data does not have a time component. We also assume we already validated and fixed hyperparameters for the first level models (models).\n",
    "\n",
    "a) Simple holdout scheme\n",
    "\n",
    "Split train data into three parts: partA and partB and partC.\n",
    "Fit N diverse models on partA, predict for partB, partC, test_data getting meta-features partB_meta, partC_meta and test_meta respectively.\n",
    "Fit a metamodel to a partB_meta while validating its hyperparameters on partC_meta.\n",
    "When the metamodel is validated, fit it to [partB_meta, partC_meta] and predict for test_meta.\n",
    "\n",
    "b) Meta holdout scheme with OOF meta-features\n",
    "\n",
    "Split train data into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in train_data we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them train_meta.\n",
    "Fit models to whole train data and predict for test data. Let's call these features test_meta.\n",
    "Split train_meta into two parts: train_metaA and train_metaB. Fit a meta-model to train_metaA while validating its hyperparameters on train_metaB.\n",
    "When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
    "\n",
    "c) Meta KFold scheme with OOF meta-features\n",
    "\n",
    "Obtain OOF predictions train_meta and test metafeatures test_meta using b.1 and b.2.\n",
    "Use KFold scheme on train_meta to validate hyperparameters for meta-model. A common practice to fix seed for this KFold to be the same as seed for KFold used to get OOF predictions.\n",
    "When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
    "d) Holdout scheme with OOF meta-features\n",
    "\n",
    "Split train data into two parts: partA and partB.\n",
    "Split partA into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in partA we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them partA_meta.\n",
    "Fit models to whole partA and predict for partB and test_data, getting partB_meta and test_meta respectively.\n",
    "Fit a meta-model to a partA_meta, using partB_meta to validate its hyperparameters.\n",
    "When the meta-model is validated basically do 2. and 3. without dividing train_data into parts and then train a meta-model. That is, first get out-of-fold predictions train_meta for the train_data using models. Then train models on train_data, predict for test_data, getting test_meta. Train meta-model on the train_meta and predict for test_meta.\n",
    "\n",
    "e) KFold scheme with OOF meta-features\n",
    "\n",
    "To validate the model we basically do d.1 -- d.4 but we divide train data into parts partA and partB M times using KFold strategy with M folds.\n",
    "When the meta-model is validated do d.5.\n",
    "\n",
    "Validation in presence of time component\n",
    "\n",
    "f) KFold scheme in time series\n",
    "\n",
    "In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n",
    "\n",
    "Split the train data into chunks of duration T. Select first M chunks.\n",
    "Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features.\n",
    "g) KFold scheme in time series with limited amount of data\n",
    "\n",
    "We may often encounter a situation, where scheme f) is not applicable, especially with limited amount of data. For example, when we have only years 2014, 2015, 2016 in train and we need to predict for a whole year 2017 in test. In such cases scheme c) could be of help, but with one constraint: KFold split should be done with the respect to the time component. For example, in case of data with several years we would treat each year as a fold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
